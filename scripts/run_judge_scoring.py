"""Score existing Set A+B outputs for mode purity using Claude Opus 4.6.

Blind scoring: the judge sees the text and topic but NOT the system prompt
or intended mode. For each text, the judge rates it on 5 mode dimensions
(1-5) and classifies the primary mode.

Usage:
    python scripts/run_judge_scoring.py
    python scripts/run_judge_scoring.py --resume  # resume from partial run

Outputs: outputs/judge_scores.json
"""

from __future__ import annotations

import argparse
import json
import logging
import os
import sys
import time
from datetime import datetime, timezone
from pathlib import Path
from typing import Any

import anthropic
import numpy as np
from scipy.spatial.distance import cdist
from sklearn.preprocessing import StandardScaler

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(message)s",
    datefmt="%H:%M:%S",
)
logger = logging.getLogger(__name__)

# ── Paths ────────────────────────────────────────────────────────────────────

PROJECT_ROOT = Path(__file__).resolve().parent.parent

# Import paths from config to respect run versioning
sys.path.insert(0, str(PROJECT_ROOT))
from config import OUTPUTS_DIR as _RUN_DIR, SIGNATURES_DIR as _SIG_DIR, PROCESSING_MODES

METADATA_PATH = _RUN_DIR / "metadata.json"
SIGNATURES_DIR = _SIG_DIR
OUTPUT_PATH = _RUN_DIR / "judge_scores.json"

# ── Constants ────────────────────────────────────────────────────────────────

JUDGE_MODEL = "claude-opus-4-6"
# Modes already imported from config above — keeps judge in sync with experiment
VALID_MODES = list(PROCESSING_MODES.keys())
MAX_RETRIES = 3
RETRY_DELAY_BASE = 2.0  # exponential backoff base in seconds
CALL_DELAY = 0.5  # delay between successful API calls

# Judge descriptions for each mode — richer than the generation prompts,
# describing what each mode looks like in output text
JUDGE_MODE_DESCRIPTIONS: dict[str, str] = {
    "structured": (
        "Organized, systematic, methodical. Uses numbered sections, headers, "
        "bullet points. Presents ideas in logical order, defines terms before "
        "using them, builds each point on the previous one. Clear formal structure."
    ),
    "associative": (
        "Stream-of-consciousness, free-flowing, non-linear. Uses fragments, "
        "dashes, ellipses. Jumps between ideas, follows tangents, connects "
        "distant concepts through metaphor and analogy. No organizing structure."
    ),
    "deliberative": (
        "Thinks out loud, weighs alternatives explicitly. Shows the reasoning "
        "process: 'on one hand... but then...' Considers then rejects possibilities. "
        "False starts, corrections, revised conclusions. Messy middle visible."
    ),
    "compressed": (
        "Maximum information density. Short sentences, no filler, no elaboration. "
        "Telegram-style. Every word earns its place. Notably shorter than other modes. "
        "Dense, factual, minimal."
    ),
    "pedagogical": (
        "Teaching voice. Starts with intuition, uses concrete examples and everyday "
        "analogies. Asks rhetorical questions. Checks comprehension: 'Does that make "
        "sense?' Builds from simple to complex. Explains jargon."
    ),
}


def _build_judge_system_prompt() -> str:
    """Build the judge system prompt dynamically from mode definitions."""
    mode_descriptions = []
    for i, (mode, desc) in enumerate(JUDGE_MODE_DESCRIPTIONS.items(), 1):
        mode_descriptions.append(f"{i}. **{mode.title()}**: {desc}")

    ratings_json = ",\n    ".join(f'"{m}": <1-5>' for m in VALID_MODES)
    mode_list = ", ".join(VALID_MODES)

    return (
        "You are an expert text analyst. You will be shown a text generated by a "
        "language model in response to a writing prompt. The model was given a "
        "specific processing mode instruction (which you do not know), and your job "
        "is to assess which processing mode(s) the text exhibits.\n\n"
        "Rate the text on each of these dimensions from 1 (not at all present) "
        "to 5 (strongly and consistently present throughout):\n\n"
        + "\n\n".join(mode_descriptions)
        + "\n\n"
        "After rating, classify which single mode BEST describes the overall text.\n\n"
        "Respond with ONLY a JSON object (no markdown fencing, no commentary):\n"
        "{\n"
        '  "ratings": {\n'
        f"    {ratings_json}\n"
        "  },\n"
        f'  "primary_mode": "<one of: {mode_list}>",\n'
        '  "confidence": <1-5>,\n'
        '  "reasoning": "<1 sentence explanation>"\n'
        "}"
    )


JUDGE_SYSTEM_PROMPT = _build_judge_system_prompt()


# ── Scoring ──────────────────────────────────────────────────────────────────


def build_user_message(topic: str, text: str) -> str:
    """Build the user message for the judge. No system prompt revealed."""
    return f"**Writing prompt:** {topic}\n\n**Generated text:**\n\n{text}"


def parse_judge_response(raw: str) -> dict[str, Any] | None:
    """Parse and validate the judge's JSON response.

    Returns None if parsing or validation fails.
    """
    # Strip markdown code fencing if present
    cleaned = raw.strip()
    if cleaned.startswith("```"):
        # Remove opening fence (with optional language tag)
        first_newline = cleaned.index("\n")
        cleaned = cleaned[first_newline + 1 :]
    if cleaned.endswith("```"):
        cleaned = cleaned[: -len("```")]
    cleaned = cleaned.strip()

    try:
        data = json.loads(cleaned)
    except json.JSONDecodeError:
        # Try to repair truncated JSON where a string field was cut off
        # This handles cases where max_tokens truncated the response mid-field
        import re
        # Find the last string value that was being written
        # Pattern: "field_name": "value_that_was_cut_off...
        repair_match = re.search(
            r'("reasoning"\s*:\s*")', cleaned
        )
        if repair_match:
            truncated = cleaned[:repair_match.end()] + '(truncated)"}'
            try:
                data = json.loads(truncated)
            except json.JSONDecodeError:
                # Also try closing with confidence field missing
                truncated2 = cleaned[:repair_match.end()] + '(truncated)"}'
                try:
                    data = json.loads(truncated2)
                except json.JSONDecodeError:
                    return None
        else:
            return None

    # Validate structure
    if not isinstance(data, dict):
        return None

    ratings = data.get("ratings")
    if not isinstance(ratings, dict):
        return None

    for mode in VALID_MODES:
        val = ratings.get(mode)
        if not isinstance(val, (int, float)) or not (1 <= val <= 5):
            return None

    primary = data.get("primary_mode")
    if primary not in VALID_MODES:
        return None

    confidence = data.get("confidence")
    if not isinstance(confidence, (int, float)) or not (1 <= confidence <= 5):
        # If response was truncated, confidence may be missing — default to 3
        if confidence is None:
            confidence = 3
            logger.debug("Confidence missing (likely truncated response), defaulting to 3")
        else:
            return None

    reasoning = data.get("reasoning", "")
    if not isinstance(reasoning, str):
        reasoning = str(reasoning)

    return {
        "ratings": {m: int(ratings[m]) for m in VALID_MODES},
        "primary_mode": primary,
        "confidence": int(confidence),
        "reasoning": reasoning,
    }


def score_single_text(
    client: anthropic.Anthropic,
    topic: str,
    text: str,
) -> dict[str, Any] | None:
    """Score a single text with retries. Returns parsed response or None."""
    user_msg = build_user_message(topic, text)

    for attempt in range(MAX_RETRIES):
        try:
            response = client.messages.create(
                model=JUDGE_MODEL,
                max_tokens=800,
                system=JUDGE_SYSTEM_PROMPT,
                messages=[{"role": "user", "content": user_msg}],
            )
            raw_text = response.content[0].text
            parsed = parse_judge_response(raw_text)

            if parsed is not None:
                return parsed

            logger.warning(
                f"Attempt {attempt + 1}/{MAX_RETRIES}: failed to parse response. "
                f"Raw: {raw_text[:200]}"
            )

        except anthropic.RateLimitError:
            wait = RETRY_DELAY_BASE ** (attempt + 1)
            logger.warning(f"Rate limited, waiting {wait:.0f}s...")
            time.sleep(wait)
        except anthropic.APIError as e:
            wait = RETRY_DELAY_BASE ** (attempt + 1)
            logger.warning(f"API error: {e}. Retrying in {wait:.0f}s...")
            time.sleep(wait)

    return None


# ── Analysis ─────────────────────────────────────────────────────────────────


def compute_summary(scores: list[dict[str, Any]]) -> dict[str, Any]:
    """Compute summary statistics from judge scores."""
    n = len(scores)
    if n == 0:
        return {"error": "No scores to summarize"}

    # Overall classification accuracy
    correct = sum(1 for s in scores if s["correct"])
    overall_accuracy = correct / n

    # Per-mode accuracy
    per_mode_accuracy: dict[str, dict[str, Any]] = {}
    for mode in VALID_MODES:
        mode_scores = [s for s in scores if s["mode_intended"] == mode]
        if mode_scores:
            mode_correct = sum(1 for s in mode_scores if s["correct"])
            per_mode_accuracy[mode] = {
                "accuracy": mode_correct / len(mode_scores),
                "n": len(mode_scores),
                "n_correct": mode_correct,
            }

    # Mean purity
    purities = [s["mode_purity"] for s in scores]
    mean_purity = float(np.mean(purities))
    std_purity = float(np.std(purities))

    # Mean judge confidence
    confidences = [s["judge_confidence"] for s in scores]
    mean_confidence = float(np.mean(confidences))

    # Confusion matrix: rows = intended, cols = judge predicted
    confusion = np.zeros((5, 5), dtype=int)
    for s in scores:
        intended_idx = VALID_MODES.index(s["mode_intended"])
        predicted_idx = VALID_MODES.index(s["primary_mode"])
        confusion[intended_idx, predicted_idx] += 1

    # Purity by correctness
    correct_purities = [s["mode_purity"] for s in scores if s["correct"]]
    incorrect_purities = [s["mode_purity"] for s in scores if not s["correct"]]

    return {
        "overall_accuracy": overall_accuracy,
        "per_mode_accuracy": per_mode_accuracy,
        "mean_purity": mean_purity,
        "std_purity": std_purity,
        "mean_confidence": mean_confidence,
        "confusion_matrix": confusion.tolist(),
        "confusion_labels": VALID_MODES,
        "mean_purity_when_correct": float(np.mean(correct_purities)) if correct_purities else 0.0,
        "mean_purity_when_incorrect": float(np.mean(incorrect_purities)) if incorrect_purities else 0.0,
    }


def compute_purity_signature_correlation(
    scores: list[dict[str, Any]],
) -> dict[str, Any] | None:
    """Correlate judge purity scores with signature-space discriminability.

    For each scored text, compute its cosine distance to its mode's centroid
    in signature space. Then correlate with judge purity.
    """
    if not SIGNATURES_DIR.exists():
        logger.info("No signatures directory found, skipping correlation analysis")
        return None

    # Load signatures for scored generation IDs
    gen_ids = [s["generation_id"] for s in scores]
    features_by_id: dict[int, np.ndarray] = {}
    for gid in gen_ids:
        npz_path = SIGNATURES_DIR / f"gen_{gid:03d}.npz"
        if npz_path.exists():
            try:
                data = np.load(npz_path)
                features_by_id[gid] = data["features"]
            except Exception as e:
                logger.warning(f"Failed to load {npz_path}: {e}")

    if len(features_by_id) < 10:
        logger.info(f"Only {len(features_by_id)} signatures loaded, skipping correlation")
        return None

    # Build feature matrix and mode labels for loaded signatures
    scored_with_sigs = [s for s in scores if s["generation_id"] in features_by_id]
    if len(scored_with_sigs) < 10:
        return None

    X = np.stack([features_by_id[s["generation_id"]] for s in scored_with_sigs])
    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    modes = [s["mode_intended"] for s in scored_with_sigs]
    purities = np.array([s["mode_purity"] for s in scored_with_sigs])

    # Compute per-sample distance to mode centroid
    centroids: dict[str, np.ndarray] = {}
    for mode in VALID_MODES:
        mask = np.array([m == mode for m in modes])
        if mask.sum() > 0:
            centroids[mode] = X_scaled[mask].mean(axis=0)

    if len(centroids) < 5:
        return None

    distances_to_centroid = []
    for i, s in enumerate(scored_with_sigs):
        mode = s["mode_intended"]
        centroid = centroids[mode].reshape(1, -1)
        sample = X_scaled[i].reshape(1, -1)
        d = float(cdist(sample, centroid, metric="cosine")[0, 0])
        distances_to_centroid.append(d)

    distances_arr = np.array(distances_to_centroid)

    # Pearson correlation: purity vs distance to centroid
    # High purity + low distance = mode-pure texts cluster well (negative correlation expected)
    if np.std(purities) < 1e-10 or np.std(distances_arr) < 1e-10:
        correlation = 0.0
    else:
        correlation = float(np.corrcoef(purities, distances_arr)[0, 1])

    # Also check: do high-purity texts have higher RF-like discriminability?
    # Split into high/low purity and compare mean distances
    median_purity = float(np.median(purities))
    high_mask = purities >= median_purity
    low_mask = purities < median_purity

    return {
        "n_samples": len(scored_with_sigs),
        "purity_distance_correlation": correlation,
        "interpretation": (
            "Negative = high-purity texts are closer to their mode centroid (good). "
            "Positive = high-purity texts are farther (bad — features not capturing what judge sees)."
        ),
        "high_purity_mean_distance": float(distances_arr[high_mask].mean()) if high_mask.sum() > 0 else 0.0,
        "low_purity_mean_distance": float(distances_arr[low_mask].mean()) if low_mask.sum() > 0 else 0.0,
        "median_purity_split": median_purity,
    }


# ── Main ─────────────────────────────────────────────────────────────────────


def load_existing_scores() -> list[dict[str, Any]]:
    """Load previously saved scores for resume functionality."""
    if OUTPUT_PATH.exists():
        try:
            with open(OUTPUT_PATH) as f:
                data = json.load(f)
            return data.get("scores", [])
        except (json.JSONDecodeError, KeyError):
            return []
    return []


def save_results(
    scores: list[dict[str, Any]],
    failed_ids: list[int],
    summary: dict[str, Any] | None = None,
    correlation: dict[str, Any] | None = None,
) -> None:
    """Save scoring results to disk."""
    output = {
        "model": JUDGE_MODEL,
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "n_scored": len(scores),
        "n_failed": len(failed_ids),
        "failed_generation_ids": failed_ids,
        "scores": scores,
        "summary": summary or {},
        "purity_signature_correlation": correlation,
    }
    OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)
    with open(OUTPUT_PATH, "w") as f:
        json.dump(output, f, indent=2, default=str)
    logger.info(f"Saved results to {OUTPUT_PATH}")


def print_summary(summary: dict[str, Any]) -> None:
    """Print a human-readable summary to stdout."""
    print("\n" + "=" * 60)
    print("JUDGE SCORING RESULTS")
    print("=" * 60)

    print(f"\nOverall classification accuracy: {summary['overall_accuracy']:.1%}")
    print(f"Mean judge confidence: {summary['mean_confidence']:.1f}/5")
    print(f"Mean mode purity: {summary['mean_purity']:.2f} (std={summary['std_purity']:.2f})")

    print(f"\nPurity when correct: {summary['mean_purity_when_correct']:.2f}")
    print(f"Purity when incorrect: {summary['mean_purity_when_incorrect']:.2f}")

    print("\nPer-mode accuracy:")
    for mode in VALID_MODES:
        stats = summary["per_mode_accuracy"].get(mode, {})
        acc = stats.get("accuracy", 0)
        n = stats.get("n", 0)
        nc = stats.get("n_correct", 0)
        print(f"  {mode:12s}: {acc:.0%} ({nc}/{n})")

    print("\nConfusion matrix (rows=intended, cols=judge predicted):")
    labels = summary["confusion_labels"]
    header = "            " + "  ".join(f"{m[:4]:>4s}" for m in labels)
    print(header)
    matrix = summary["confusion_matrix"]
    for i, row in enumerate(matrix):
        row_str = "  ".join(f"{v:4d}" for v in row)
        print(f"  {labels[i]:10s}  {row_str}")

    print()


def print_correlation(correlation: dict[str, Any] | None) -> None:
    """Print purity-signature correlation results."""
    if correlation is None:
        print("Purity-signature correlation: skipped (no signatures available)")
        return

    print(f"Purity-signature correlation (n={correlation['n_samples']}):")
    print(f"  Pearson r (purity vs centroid distance): {correlation['purity_distance_correlation']:.3f}")
    print(f"  High-purity mean distance to centroid: {correlation['high_purity_mean_distance']:.4f}")
    print(f"  Low-purity mean distance to centroid:  {correlation['low_purity_mean_distance']:.4f}")
    print(f"  {correlation['interpretation']}")
    print()


def run_judge_scoring(resume: bool = False) -> None:
    """Main entry point."""
    # Validate API key
    api_key = os.environ.get("ANTHROPIC_API_KEY")
    if not api_key:
        logger.error("ANTHROPIC_API_KEY not set. Export it before running.")
        sys.exit(1)

    # Load metadata
    if not METADATA_PATH.exists():
        logger.error(f"Metadata not found: {METADATA_PATH}")
        sys.exit(1)

    with open(METADATA_PATH) as f:
        metadata = json.load(f)

    # Filter to Set A+B
    ab_generations = [
        g for g in metadata["generations"]
        if g["prompt_set"] in ("A", "B")
    ]
    logger.info(f"Found {len(ab_generations)} Set A+B generations to score")

    # Handle resume
    existing_scores: list[dict[str, Any]] = []
    scored_ids: set[int] = set()
    if resume:
        existing_scores = load_existing_scores()
        scored_ids = {s["generation_id"] for s in existing_scores}
        logger.info(f"Resuming: {len(scored_ids)} already scored, "
                     f"{len(ab_generations) - len(scored_ids)} remaining")

    # Initialize client
    client = anthropic.Anthropic(api_key=api_key)

    # Score each text
    scores = list(existing_scores)
    failed_ids: list[int] = []
    to_score = [g for g in ab_generations if g["generation_id"] not in scored_ids]

    for i, gen in enumerate(to_score):
        gid = gen["generation_id"]
        mode = gen["mode"]
        topic = gen["topic"]
        text = gen["generated_text"]

        logger.info(
            f"[{len(scores) + 1}/{len(ab_generations)}] "
            f"Scoring gen_{gid:03d} ({mode}/{topic[:30]}...)"
        )

        result = score_single_text(client, topic, text)

        if result is None:
            logger.error(f"Failed to score gen_{gid:03d} after {MAX_RETRIES} retries")
            failed_ids.append(gid)
            continue

        # Compute mode purity: intended mode rating - mean of other ratings
        intended_rating = result["ratings"][mode]
        other_ratings = [result["ratings"][m] for m in VALID_MODES if m != mode]
        purity = intended_rating - float(np.mean(other_ratings))

        score_entry = {
            "generation_id": gid,
            "mode_intended": mode,
            "topic": topic,
            "ratings": result["ratings"],
            "primary_mode": result["primary_mode"],
            "judge_confidence": result["confidence"],
            "reasoning": result["reasoning"],
            "mode_purity": round(purity, 3),
            "correct": result["primary_mode"] == mode,
        }
        scores.append(score_entry)

        # Save incrementally every 10 texts
        if (len(scores) % 10 == 0) or (i == len(to_score) - 1):
            save_results(scores, failed_ids)

        # Rate limiting delay
        time.sleep(CALL_DELAY)

    # Final analysis
    logger.info("Computing summary statistics...")
    summary = compute_summary(scores)

    logger.info("Computing purity-signature correlation...")
    correlation = compute_purity_signature_correlation(scores)

    # Save final results
    save_results(scores, failed_ids, summary, correlation)

    # Print results
    print_summary(summary)
    print_correlation(correlation)

    if failed_ids:
        print(f"WARNING: {len(failed_ids)} generations failed scoring: {failed_ids}")
        print("Run with --resume to retry them.")

    # Diagnostic interpretation
    acc = summary["overall_accuracy"]
    print("=" * 60)
    print("DIAGNOSTIC INTERPRETATION")
    print("=" * 60)
    if acc >= 0.80:
        print(f"Judge accuracy {acc:.0%} >= 80%: Elicitation is WORKING.")
        print("The texts are clearly mode-differentiated. The bottleneck is")
        print("likely in feature extraction — the computational signatures")
        print("aren't capturing what the judge can see in the text.")
        print("→ Prioritize feature engineering (Priority 4).")
    elif acc >= 0.60:
        print(f"Judge accuracy {acc:.0%}: MODERATE elicitation quality.")
        print("Some modes are well-expressed, others aren't. Check per-mode")
        print("accuracy to identify which modes need prompt improvement.")
        print("→ Targeted prompt sharpening for weak modes, plus feature work.")
    else:
        print(f"Judge accuracy {acc:.0%} < 60%: Elicitation is WEAK.")
        print("The texts aren't clearly mode-differentiated at the surface level.")
        print("Before improving features, improve the prompts.")
        print("→ Prioritize prompt sharpening (Priority 3).")
    print()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Judge scoring of mode purity")
    parser.add_argument(
        "--resume", action="store_true",
        help="Resume from partial run (skip already-scored texts)",
    )
    args = parser.parse_args()
    run_judge_scoring(resume=args.resume)
